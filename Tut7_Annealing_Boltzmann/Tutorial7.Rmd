---
title: "Tutorial 7 Solution"
subtitle: "Pattern Recognition"
author: "Markus Löning, Simon Weiß & Tamara Bogatzki"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: html_document
---

###1a) Is it reasonable that a higher energy is sometimes accepted in stochastic simulated annealing? Why? Why does it converge? Compare to greedy algorithm.
With a greedy algorithm only configurations which lead to lower energy are accepted. However, since the change in energy depends only on nodes (locally) connected to node $i$ the greedy algorithm is likely to get caught in local minima.

With stochastic simulated annealing, sometimes a configuration with higher energy $Eb$ is accepted. This is reasonable because it allows the system to "jump" to different areas of the energy landscape. Thus, avoiding to get caught in local minima. Generally, the probability of "jumps" to different areas decreases with "temperature" $T(k)$. 
This also ensures that the alogrithm converges. With low $T$ (which decreases in $k$) the probability of accepting a configuration with higher energy decreases. Hence, the algorithm becomes more "greedy" witch decreasing temperature. 

In the end, the configuration which yielded the lowest energy throught the algorithm is chosen.

###1b) Stochastic Simulated Annealing
```{r}
#Stochastic simulated annealing
anneal <- function(w, s1, t1, c, max_iter){
  
  #initialise values
  num_iter <- 0 #starting iteration
  s <- s1 #initial states
  t <- t1 #initial temperature
  t_hist <- t1 #initialise storage vector for temperature hisbtory
  E_hist <- -0.5 * s %*% w %*% s #calculate initial energy + initialise energy history storage vector
  s_hist <- s1 #history of configurations. Allows to extract configuration with lowest energy throught the algorithm
  
  #Loop until max_iter is reached
  while (num_iter < max_iter) {
    num_iter <- num_iter + 1 #increase iteration counter by 1
    i <- sample(length(s), 1) #randomly select 1 state
    #Calculate "local" energy
    Ea <- -0.5*s[i]*w[i,-i] %*% s[-i]
    Eb <- -Ea #energy with alternative state where s[i] = -s[i]
    #Adjust states
    if (Eb<Ea){
      s[i]<- -s[i]
    } else {             #Eb higher energy than Ea
        if(exp(-(Eb-Ea)/t) > runif(1, 0, 1)){
        s[i] <- -s[i]
        }
    }
    #Calculate total energy of the system
    E <- -0.5 * s %*% w %*% s
    E_hist[num_iter + 1] <- E
    #Annealing schedule
    t <- c*t 
    t_hist[num_iter + 1] <- t
    s_hist <- cbind(s_hist, s)
  }

#Determine the configuration with the lowest energy achieved throught the algorithm
s <- s_hist[,which.min(E_hist)]
#Associated lowest energy achieved
E <- min(E_hist)
  
  #Function outputs
  return(list(states = s, energy = E, energy_hist = E_hist, t_hist = t_hist, num_iter = num_iter, s_hist <- s_hist))
} 
```

```{r}
#weights: symmetric, with self-feedback terms = 0
w <- matrix(c(0, 5, -3, 4, 4, 1, 
              5, 0, -1, 2, -3, 1,
              -3, -1, 0, 2, 2, 0, 
              4, 2, 2, 0, 3, -3,
              4, -3, 2, 3, 0, 5,
              1, 1, 0, -3, 5, 0),
            nrow = 6, ncol = 6)

#initial states
s1 <- c(1, -1, -1, 1, -1, 1)

#algorithm parameters
t1 <- 10 #high starting temperature
c <- 0.9 #cooling parameter

set.seed(123) #set seed for random polling
anneal_est <- anneal(w, s1, t1, c, max_iter = 100)
```



###1d) Deterministic Simulated Annealing
```{r}
#Deterministic simulated annealing
anneal.det <- function(w, s1, t1, c, max_iter){
  
  #initialise values
  num_iter <- 0 #starting iteration
  s <- s1 #initial states
  t <- t1 #initial temperature
  t_hist <- t1 #initialise storage vector for temperature history
  E_hist <- -0.5 * s %*% w %*% s #calculate initial energy + initialise energy history storage vector
  s_hist <- s1
  
  #Loop until max_iter is reached
  while (num_iter < max_iter) {
    num_iter <- num_iter + 1 #increase iteration counter by 1
    i <- sample(length(s), 1) #randomly select 1 state
    #Calculate force exerted on node i
    l <- w[i,-i] %*% s[-i]
    #Adjust state according to response function
    s[i] <- tanh(l/t)
    #Force states to be non-zero
    if (s[i] == 0) {
      ifelse(s_hist[i,num_iter] > 0, s[i] <- 1, s[i] <- -1)}
    #Calculate total energy of the system
    E <- -0.5 * s %*% w %*% s
    E_hist[num_iter + 1] <- E
    #Annealing schedule
    t <- c*t 
    t_hist[num_iter + 1] <- t
    s_hist <- cbind(s_hist, s)
  }
  
#Determine the configuration with the lowest energy achieved throught the algorithm
s <- s_hist[,which.min(E_hist)]
#Associated lowest energy achieved
E <- min(E_hist)

  #Function outputs
  return(list(states = s, energy = E, energy_hist = E_hist, t_hist = t_hist, num_iter = num_iter, s_hist = s_hist))
} 

```

```{r}
set.seed(123)
anneal.det_est <- anneal.det(w, s1, t1, c, max_iter = 100)
```

##Exercise 2: Boltzmann learning
###2a
Stochastic Boltzmann learning is a method to learn the appropriate weights for classification for a network like the one used in Exercise 1. The weight update is determined by the learning rate, temperature, and gradient descent in the relative entropy. This minimizes the Kullback-Leibler divergence between the actual and desired probability distribution of a given state.

The teacher component (also learning component) is the correlation of the variables $s_i$ and $s_j$ when the visible units are held fixed (clamped). The name comes from the fact that the visible units are held to values given by the teacher (labeld training pattern).
In the student component (or unlearning component), in contrast, the variables are free to vary. This reduces spurious correlations between the units. Spurious correlations are those correlations that are not due to the presented training patterns. The desired weights are achieved if the student and teacher component are equal.

###2b
Pattern completition aims to estimate the full pattern from just a part of that pattern; e.g. estimate the true hand written number from an incomplete picture of a number. Instead of classifying the image of a hand written symbol as one of 10 possible numbers, the algorithm tries to complete the drawing on the image. This is related to classification with missing features. For both cases Boltzmann training is the preferred method. When classfying with missing features, input units corresponding to missing input features are allowed to vary. They are treated as unclamped hidden units instead of clamped visible input units. As a result, those units assume values consistent with the rest of the input pattern and the current state of the network during annealing. In pattern completion, when a deficient pattern is presented, the units in a subset of the visible unites are clamped to the components of the partial pattern. After annealing the network the estimated unkown features appear on the remaining visibile units.


